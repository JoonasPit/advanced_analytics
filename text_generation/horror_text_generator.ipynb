{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3da5832e-69e5-440d-b6ad-b584d79935fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-23 19:40:44.161302: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-23 19:40:44.171119: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1732383644.182844  596382 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1732383644.186387  596382 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-23 19:40:44.198440: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "from urllib import request\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from io import StringIO\n",
    "from collections import Counter\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "import nltk\n",
    "from __future__ import print_function\n",
    "from keras.layers import Dense, Activation, SimpleRNN, LSTM\n",
    "from keras.models import Sequential\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import wordcloud as w\n",
    "import matplotlib.pyplot as plot\n",
    "import torch\n",
    "from transformers import  BartTokenizer, TFBartForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "a8315135-1231-4296-89ce-db618b8a38de",
   "metadata": {},
   "outputs": [],
   "source": [
    "shelley_frank = request.urlopen('https://www.gutenberg.org/cache/epub/84/pg84.txt').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "32edb3cc-6664-47d5-860b-5e3548b2e2c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Project Gutenberg eBook of Frankenstein; Or, The Modern Prometheus\n",
      "    \n",
      "This ebook is for the \n"
     ]
    }
   ],
   "source": [
    "shelley_frank = shelley_frank.decode('utf-8-sig')\n",
    "print(shelley_frank[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "09e85a4f-09df-47b1-8f97-202a9c378970",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_start = 'You will rejoice'\n",
    "word_index = shelley_frank.find(actual_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "e288336b-6c18-46e3-8782-c66115e02a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if word_index != -1:\n",
    "    shelley_frank = shelley_frank[word_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "d8b0754c-d990-4d5c-b383-fdcbf10c450c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you will rejoice to hear that no disaster has accompanied the commencement of an enterprise which yo\n"
     ]
    }
   ],
   "source": [
    "shelley_frank = re.sub(r'—', ' ', shelley_frank.lower())\n",
    "shelley_frank = shelley_frank.replace('project gutenberg', '')\n",
    "shelley_frank = nltk.word_tokenize(shelley_frank)\n",
    "shelley_frank = ' '.join(shelley_frank)  \n",
    "\n",
    "# Ensure space after punctuation if it's missing\n",
    "print(shelley_frank[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "c7976306-1b83-4e23-b136-d0679c13a91c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you will rejoice to hear that no disaster has accompanied the commencement of an enterprise which you have regarded with such evil forebodings. i arrived here yesterday, and my first task is to assure my dear sister of my welfare and increasing confidence in the success of my undertaking. i am already far north of london, and as i walk in the streets of petersburgh, i feel a cold northern breeze play upon my cheeks, which braces my nerves and fills me with delight. do you understand this feeling? this breeze, which has travelled from the regions towards which i am advancing, gives me a foretaste of those icy climes. inspirited by this wind of promise, my daydreams become more fervent and vivid. i try in vain to be persuaded that the pole is the seat of frost and desolation; it ever presents itself to my imagination as the region of beauty and delight. there, margaret, the sun is for ever visible, its broad disk just skirting the horizon and diffusing a perpetual splendour. there for wi\n"
     ]
    }
   ],
   "source": [
    "shelley_frank = re.sub(r'[^a-zA-Z.,!?; ]', '', shelley_frank)\n",
    "shelley_frank = re.sub(r'([.,!?;])', r'\\1 ', shelley_frank)\n",
    "shelley_frank = re.sub(r'\\s+([.,!?;])', r'\\1', shelley_frank)\n",
    "shelley_frank = re.sub(r'\\s+', ' ', shelley_frank) \n",
    "print(shelley_frank[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "740019aa-d2ad-4797-a25d-2fea45c70cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./shelley.txt', 'w') as output_file:\n",
    "    output_file.write(shelley_frank)\n",
    "\n",
    "with open('./shelley.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "sentences = re.split(r'\\.|\\n', text)  # This will split by periods and newlines\n",
    "sentences = [sentence.strip() for sentence in sentences if sentence.strip()]\n",
    "\n",
    "with open('./shelley_processed.txt', 'w') as file:\n",
    "    for sentence in sentences:\n",
    "        file.write(sentence + '\\n')\n",
    "\n",
    "# Print the number of lines\n",
    "#print(len(lines))  # Make sure it prints a value greater than 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "385313f2-e52a-4ba7-9f30-eff4003b581a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "CPU times: user 1.55 s, sys: 366 ms, total: 1.92 s\n",
      "Wall time: 175 ms\n",
      "mkdir: cannot create directory ‘./transf_shelley’: File exists\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['./transf_shelley/vocab.json', './transf_shelley/merges.txt']"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tokenizers import ByteLevelBPETokenizer\n",
    "from tokenizers.implementations import ByteLevelBPETokenizer\n",
    "from tokenizers.processors import BertProcessing\n",
    "\n",
    "bpe_tokenizer = ByteLevelBPETokenizer()\n",
    "\n",
    "trainer = trainers.BpeTrainer(vocab_size=20000, min_frequency=2, special_tokens=[\"<unk>\", \"<s>\", \"</s>\", \"<pad>\", \"<mask>\"])\n",
    "\n",
    "%time bpe_tokenizer.train(files='./shelley_processed.txt', vocab_size=20000, min_frequency=2)\n",
    "!mkdir ./transf_shelley\n",
    "model_name = './transf_shelley'\n",
    "bpe_tokenizer.save_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "46bb80eb-b9db-4d8e-8921-09a8e68b00af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PretrainedConfig\n",
    "\n",
    "# Create configuration for the tokenizer (for future use)\n",
    "from transformers import GPT2Config\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "# Define the model configuration for a causal language model\n",
    "config = GPT2Config(\n",
    "    vocab_size=20000,                # Set vocab size based on your tokenizer\n",
    "    n_positions=512,                 # Max sequence length\n",
    "    n_ctx=512,                       # Context window\n",
    "    n_embd=768,                      # Embedding size\n",
    "    n_layer=12,                      # Number of transformer layers\n",
    "    n_head=12,                       # Number of attention heads\n",
    "    pad_token_id=50256,              # Default pad token (used in GPT2)\n",
    "    bos_token_id=50256,              # Beginning of sentence token (GPT2)\n",
    "    eos_token_id=50256,              # End of sentence token (GPT2)\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_config(config)\n",
    "#model.train()\n",
    "\n",
    "config.save_pretrained(model_name)\n",
    "# After training, save the model weights and configuration\n",
    "model.save_pretrained(model_name)\n",
    "\n",
    "# Save the configuration to the same directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "86ada377-5284-4b62-ae70-2e130e1dcb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "use_gpu = -1\n",
    "if torch.cuda.is_available():\n",
    "    use_gpu = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "b5a5ade2-65c5-426a-ab72-04f189fb222c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "616eb2b811dc41b2a4a265f983b79b2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "datasets = load_dataset(\"text\", data_files={\"train\": './shelley_processed.txt'})\n",
    "datasets = datasets[\"train\"].train_test_split(test_size=0.2)\n",
    "train_dataset = datasets['train']\n",
    "eval_dataset = datasets['test']\n",
    "#model_checkpoint = 'stabilityai/stablelm-zephyr-3b'\n",
    "model_checkpoint = model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "c3e96a05-3053-4e73-8666-513a67723449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 2474\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(datasets['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "19d85465-48ed-4c24-ab11-3c6ad525a4e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "w_tokenizer = ByteLevelBPETokenizer(\n",
    "    './shelley_safe/vocab.json',\n",
    "    'shelley_safe/merges.txt'\n",
    ")\n",
    "\n",
    "# Use the directory where you saved the tokenizer files\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "cbdf4270-b2cf-4342-9d8f-ccef76777e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(examples):\n",
    "    return tokenizer(examples['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "d2f1b70d-e077-442b-b1d8-d2e38baf1d22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef8abe0c7c4e4d93a84581886e41d55d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2474 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5106ce2344124bfb92e93f771bbc58c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/619 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_data = datasets.map(tokenize, batched = True, num_proc = 1, remove_columns = ['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "b1937702-d531-47f0-9fca-c4234a2cc991",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [393,\n",
       "  3453,\n",
       "  376,\n",
       "  984,\n",
       "  1157,\n",
       "  292,\n",
       "  3651,\n",
       "  1415,\n",
       "  873,\n",
       "  7648,\n",
       "  281,\n",
       "  386,\n",
       "  451,\n",
       "  1149,\n",
       "  421,\n",
       "  352,\n",
       "  2357,\n",
       "  11,\n",
       "  350,\n",
       "  259,\n",
       "  5557,\n",
       "  343,\n",
       "  317,\n",
       "  259,\n",
       "  524,\n",
       "  7607,\n",
       "  2189],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1]}"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_data['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "3c7f7821-4931-4d9f-929f-7233fc32a3d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c895429f76a46cb8e9b657e60de9e0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2474 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9667ee5f506348eeb9e8bb47d9066d5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/619 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "' i said, little happiness remains for us on earth; yet all that i may one day enjoy is centred in youthe sweet girl welcomed me with warm affection, yet tears were in her eyes as she beheld my emaciated frame and feverish cheeksyet when she died! nay, then i was not miserablelisten to me, frankensteini passed an hour in this state of mind, when suddenly i reflected how fearful the combat which i momentarily expected would be to my wife, and i earnestly entreated her to retire, resolving not to join her until i had obtained some knowledge as to the situation of my enemyunable to endure the aspect of the being'"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hyperparams\n",
    "block_size = 128\n",
    "\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "lm_datasets = tokenized_data.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    batch_size=1000,\n",
    "    num_proc=1,\n",
    ")\n",
    "\n",
    "tokenizer.decode(lm_datasets[\"train\"][1][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "bdfd494e-f343-48d8-a71c-a4c6a4b7bf76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "model = AutoModelForCausalLM.from_pretrained(model_checkpoint)\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "#model_name = model_checkpoint.split(\"/\")[-1]\n",
    "training_args = TrainingArguments(\n",
    "    f\"{model_name}-test\",\n",
    "    per_device_train_batch_size=2,  \n",
    "    gradient_accumulation_steps=4,  \n",
    "    eval_strategy = \"epoch\",\n",
    "    num_train_epochs = 50,\n",
    "    learning_rate=0.01,\n",
    "    weight_decay=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "401e937c-653a-4d1e-879c-dc2de4a2bbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=lm_datasets[\"train\"],\n",
    "    eval_dataset=lm_datasets[\"test\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "dcdbb99f-c3b0-47af-8dc0-9f506b5d9117",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3450' max='3450' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3450/3450 06:10, Epoch 50/50]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>6.641420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>6.509946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>6.506912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>6.482923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>6.447932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>6.446107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>No log</td>\n",
       "      <td>6.476734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>6.331600</td>\n",
       "      <td>6.452150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>6.331600</td>\n",
       "      <td>6.450957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>6.331600</td>\n",
       "      <td>6.542045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>6.331600</td>\n",
       "      <td>6.596794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>6.331600</td>\n",
       "      <td>6.585039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>6.331600</td>\n",
       "      <td>6.540209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>6.331600</td>\n",
       "      <td>6.599301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>6.098900</td>\n",
       "      <td>6.581044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>6.098900</td>\n",
       "      <td>6.586570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>6.098900</td>\n",
       "      <td>6.609122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>6.098900</td>\n",
       "      <td>6.558441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>6.098900</td>\n",
       "      <td>6.575299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>6.098900</td>\n",
       "      <td>6.573639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>6.098900</td>\n",
       "      <td>6.619307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>6.064400</td>\n",
       "      <td>6.571898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>6.064400</td>\n",
       "      <td>6.602270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>6.064400</td>\n",
       "      <td>6.613657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>6.064400</td>\n",
       "      <td>6.602838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>6.064400</td>\n",
       "      <td>6.625469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>6.064400</td>\n",
       "      <td>6.644749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>6.064400</td>\n",
       "      <td>6.623956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>5.980000</td>\n",
       "      <td>6.610701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>5.980000</td>\n",
       "      <td>6.618700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>5.980000</td>\n",
       "      <td>6.606644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>5.980000</td>\n",
       "      <td>6.629833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>5.980000</td>\n",
       "      <td>6.635418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>5.980000</td>\n",
       "      <td>6.637457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>5.980000</td>\n",
       "      <td>6.644769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>5.980000</td>\n",
       "      <td>6.650052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>5.870300</td>\n",
       "      <td>6.669169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>5.870300</td>\n",
       "      <td>6.692555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>5.870300</td>\n",
       "      <td>6.716286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>5.870300</td>\n",
       "      <td>6.682982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>5.870300</td>\n",
       "      <td>6.704335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>5.870300</td>\n",
       "      <td>6.721525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>5.870300</td>\n",
       "      <td>6.742777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>5.762200</td>\n",
       "      <td>6.727668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>5.762200</td>\n",
       "      <td>6.742332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>5.762200</td>\n",
       "      <td>6.754738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>5.762200</td>\n",
       "      <td>6.762023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>5.762200</td>\n",
       "      <td>6.765831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>5.762200</td>\n",
       "      <td>6.773403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>5.762200</td>\n",
       "      <td>6.772815</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "% time trainer.train()\n",
    "trainer.save_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "b96b70ed-f79e-401a-9168-cb556b8e3e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "s_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "s_tokenizer = AutoTokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "67d32f3a-fee3-4143-9228-1b82e84e157c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you will rejoice, for you; a me is from by me of last and was my first be on that i, my power in the sorrows you he was then? own shall soon, butbut he a creature of the moon, and i of a and his friend! the cottagers, be of her my heart the white? it me of a father and my existence to you passed your being, a dungeon and, my departure months a young i said she not and had was with so first the more be the door to the wind a child the words and a thousand, and a smiling he may to his thought and when the sea by the sight was to the nighti had i read other, which i had with an mind world, you to a miserable light of all a young was i saw and how the old when not your father a prison? and the same sun a few candour as a few object thoughts and if and her custom and hatred were a multitude, her names; but, at the wind, she of the night and their mind sky of a feeble from a suffici; the murderer of my thoughts, the arabian been my imagination which at clerval in this place by his heart in his face, and was her remainder,, i feel you was and i had might saw what had not mention at a young, i had in enter, we saw a few native was they if they can and i had was it, had already i was my sensations to that all this change on her sun, and i can not that as at a miserable; you at his audible were, and affection in my mind fiend and tur to me and hatredi the greatest a young and in all one exclamation, my fellow; and a secure of a hundred and my former immediate of my daydreams was the whole his thoughts you not to the idea was to the fiend to my own thousand of me from be of a long it, the ground for for he did the corpse by my daydreams of what was that when you not the subject and of a thousand, when they forget was i did was its heart i could enchanted of a few hands of his first, when had had not, to my daydreams it then of his father and not with his father of the most solitary, that it to his favourite to their own sensitive and desol me by my mind house of the treesi, i had would destroy of the morning for, to my eyes had have not it of life by that to take was i had been in i often am, in a personal, i would the remainder\n"
     ]
    }
   ],
   "source": [
    "#Txt gen attempr\n",
    "\n",
    "prompt = 'you will rejoice'\n",
    "\n",
    "input_ids = s_tokenizer(prompt, return_tensors='pt').input_ids\n",
    "# model generate with causal lm  = Text generation\n",
    "\n",
    "outputs = s_model.generate(\n",
    "    input_ids,\n",
    "    # Gen text len\n",
    "    max_length=512,           \n",
    "    num_return_sequences=1,\n",
    "    # Sampling should be true if using top_k or top_p word estimeation if false use greedy methods\n",
    "    do_sample=True,\n",
    "    #top_k=3,\n",
    "    top_p=0.95,\n",
    "    # Set temp for less random gen\n",
    "    temperature=1.0,\n",
    ")\n",
    "\n",
    "\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "b0dc3819-d927-4239-ad5a-fb5d33d0c339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you will rejoice, and the same first, i, and the same same of the next i was the same, and was, and the same the same little, i had was the the the own the greatest, and i had was and was i was and, i was the greatest the same, and i was and the same boat and i was i was i had was i was i was and i had was the same the same the the sun, and i was the greatest the father, and the same, and i was and i was the same object, i was and i had the same, and i had been of the same monster and i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i\n"
     ]
    }
   ],
   "source": [
    "\n",
    "greedy_search_outputs = s_model.generate(\n",
    "    input_ids,\n",
    "    # Gen text len\n",
    "    max_length=512,           \n",
    "    num_return_sequences=1,\n",
    "    temperature=1.0,\n",
    ")\n",
    "\n",
    "\n",
    "generated_text = tokenizer.decode(greedy_search_outputs[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "9f537cdf-35fe-4b72-8ce8-71f52bddc369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you will rejoice, and i was i was i had had not be, and i was i was the world, i had was i, and i was and i had, not be not have been the greatest, and i had was i had was i was i was i was i had been, i had was i had have not be be have been of my father, but, and i was i was i was i had not not have not not me, but, i had have been the whole the same, and was and i had been, and i had was and i was i was and i had not, and i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i was i\n"
     ]
    }
   ],
   "source": [
    "beam_search_outputs = s_model.generate(\n",
    "    input_ids,\n",
    "    # Gen text len\n",
    "    max_length=512,           \n",
    "    num_return_sequences=1,\n",
    "    num_beams = 2,\n",
    "    temperature=1.0,\n",
    ")\n",
    "\n",
    "\n",
    "generated_text = tokenizer.decode(beam_search_outputs[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c051b514-49cf-4f04-9ca1-0abe4dd2f699",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
